{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89f327d",
   "metadata": {},
   "source": [
    "**비디오에서 얼굴 검출하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b516841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "\n",
    "#mediapipe에서 제공하는 얼굴 감지(눈,코,입 위치) 모듈\n",
    "mp_face_detection=mp.solutions.face_detection\n",
    "#감지된 얼굴의 위치나 특징점을 화면에 그려주는 기능\n",
    "mp_drawing=mp.solutions.drawing_utils\n",
    "\n",
    "#얼굴 감지 객체\n",
    "#model_selection(0:근거리, 1:원거리)\n",
    "#min_detection_confidence(얼굴로 인정할 최소 신뢰도)\n",
    "face_detection=mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)\n",
    "\n",
    "#openCV에서 비디오 키기\n",
    "cap=cv.VideoCapture(0,cv.CAP_DSHOW)\n",
    "\n",
    "while True:\n",
    "    #ret: 프레임 읽기 성공 여부\n",
    "    ret,frame=cap.read()\n",
    "    if not ret:\n",
    "        print('프레임 획득 실패')\n",
    "        break\n",
    "    #mediapipe는 RGB형식만 지원\n",
    "    #변환된 영상을 process에 전달\n",
    "    res=face_detection.process(cv.cvtColor(frame,cv.COLOR_BGR2RGB))\n",
    "    \n",
    "    #얼굴이 감지되었을 때\n",
    "    if res.detections:\n",
    "        #감지된 얼굴을 하나씩 순회\n",
    "        for detection in res.detections:\n",
    "            #감지된 얼굴의 위치와 특징점(눈,코,입 등)을 화면 위에 시각적으로 표시\n",
    "            mp_drawing.draw_detection(frame,detection)\n",
    "\n",
    "    #flip(frame,1): 좌우 반전/ 0:상하 반전    \n",
    "    cv.imshow('MediaPipe Face Detection from video',cv.flip(frame,1))\n",
    "    if cv.waitKey(5)==ord('q'):\n",
    "        break\n",
    "\n",
    "#웹캠 장치 해제    \n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b695a84",
   "metadata": {},
   "source": [
    "**얼굴을 장식하는 증강 현실 구현하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f866c57",
   "metadata": {},
   "source": [
    "| 패키지                       | 버전            |\n",
    "| ------------------------- | ------------- |\n",
    "| **python**                | 3.9 (가상환경 버전) |\n",
    "| **tensorflow**            | 2.10.0        |\n",
    "| **keras**                 | 2.10.0        |\n",
    "| **protobuf**              | 3.20.3        |\n",
    "| **mediapipe**             | 0.10.9        |\n",
    "| **pixellib**              | 0.7.1         |\n",
    "| **opencv-python**         | 4.5.5.64      |\n",
    "| **opencv-contrib-python** | 4.11.0.86     |\n",
    "| **numpy**                 | 1.26.4        |\n",
    "| **pillow**                | 11.3.0        |\n",
    "| **imantics**              | 0.1.12        |\n",
    "| **matplotlib**            | 3.9.4         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "\n",
    "#IMREAD_UNCHANGED: 원본 그대로 읽기(투명도 포함)\n",
    "img=cv.imread('heart.png',cv.IMREAD_UNCHANGED)\n",
    "#이미지 크기를 직접 지정하지 않고 10%로 축소\n",
    "img=cv.resize(img,dsize=(0,0),fx=0.1,fy=0.1)\n",
    "#shape[1]:이미지 너비, shape[0]:높이\n",
    "w,h=img.shape[1],img.shape[0]\n",
    "\n",
    "#mediapipe에서 제공하는 얼굴 감지(눈,코,입 위치) 모듈\n",
    "mp_face_detection=mp.solutions.face_detection\n",
    "#감지된 얼굴의 위치나 특징점을 화면에 그려주는 기능\n",
    "mp_drawing=mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "#얼굴 감지 객체\n",
    "#model_selection(0:근거리, 1:원거리)\n",
    "#min_detection_confidence(얼굴로 인정할 최소 신뢰도)\n",
    "face_detection=mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)\n",
    "\n",
    "cap=cv.VideoCapture(0,cv.CAP_DSHOW)\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    #ret: 프레임 읽기 성공 여부\n",
    "    if not ret:\n",
    "        print('프레임 획득 실패')\n",
    "        break\n",
    "    \n",
    "    #mediapipe는 RGB형식만 지원\n",
    "    #변환된 영상을 process에 전달\n",
    "    res=face_detection.process(cv.cvtColor(frame,cv.COLOR_BGR2RGB))\n",
    "    \n",
    "    #얼굴이 감지되었을 때\n",
    "    if res.detections:\n",
    "        #감지된 얼굴을 하나씩 순회\n",
    "        for det in res.detections:\n",
    "            #양쪽 눈 좌표 가져오기\n",
    "            for eye_point in [mp_face_detection.FaceKeyPoint.RIGHT_EYE, mp_face_detection.FaceKeyPoint.LEFT_EYE]:\n",
    "                p = mp_face_detection.get_key_point(det, eye_point)\n",
    "                #감지된 눈 좌표(p.x)를 프레임 픽셀 좌표(가로 크기)로 변환(*frame.shape[1])\n",
    "                #불러온 이미지 중심이 눈 좌표에 오도록(-w//2)\n",
    "                x1,x2=int(p.x*frame.shape[1]-w//2),int(p.x*frame.shape[1]+w//2)\n",
    "                y1,y2=int(p.y*frame.shape[0]-h//2),int(p.y*frame.shape[0]+h//2)\n",
    "                #불러온 이미지가 프레임을 벗어나지 않을 때\n",
    "                if x1>0 and y1>0 and x2<frame.shape[1] and y2<frame.shape[0]:\n",
    "                    #이미지가 png이므로 RGBA(4채널)\n",
    "                    #/255: 알파 채널(투명도)을 0~1로 정규화 (alpha=1.0:불투명, 0.0: 완전 투명)\n",
    "                    alpha=img[:,:,3:]/255\n",
    "                    #눈 영역(frame[y1:y2,x1:x2]) * (1-alpha): alpha값이 0~1일 때 배경 픽셀과 불러온 이미지 픽셀이 혼합, alpha가 1이면 불러온 이미지가 완전히 덮음\n",
    "                    #img[:,:,:3]: RGB채널만 추출 * alpha = 불러온 이미지의 불투명한 부분은 진하게, 투명한 부분은 연하게 적용\n",
    "                    frame[y1:y2,x1:x2]=frame[y1:y2,x1:x2]*(1-alpha)+img[:,:,:3]*alpha\n",
    "                \n",
    "    cv.imshow('MediaPipe Face AR',cv.flip(frame,1))\n",
    "    if cv.waitKey(5)==ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af56e188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "texttbookk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
